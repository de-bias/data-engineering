---
title: "09_pw_location_filter_hours"
format: html
editor: visual
---

```{r}
# Load libraries
library(arrow)        # For working with Apache Arrow
library(sparklyr)    # For connecting to Spark
library(dplyr)       # For data manipulation
```

```{r}
wd <- "/Volumes/rdm04/DEBIAS/data/inputs/pickwell/uk"
```

```{r}
days <-  c(
  "01042021",
  "02042021",
  "03042021",
  "04042021",
  "05042021",
  "06042021",
  "07042021"
)

```

```{r}
for (day in days) {

    # Configure Spark
    config <- spark_config()
    config$`sparklyr.shell.driver-memory` <- '8G'
    
    # Connect to Spark
    sc <- spark_connect(master = "local", config = config)
    
    # Define the data parent folder and file pattern
    data_parent_folder <- file.path(
      wd, "geocoded-hex10", day
      )
    file_pattern <- file.path(data_parent_folder, "*-geocoded.csv.gz")
    
    # Read CSV files into Spark DataFrame
    df <- spark_read_csv(
      sc,
      path = file_pattern,
      memory = TRUE
    )
    
    # Create a new column 'combo_location' by concatenating 'NAME_2' and 'settlement_type'
    rdd_1 <- df %>% 
      mutate(
        combo_location = code_h3
      )
    
    # Filter for rows where the time is between '19:00:00' and '6:59:00'
    rdd_3 <- rdd_1 %>%
      filter((hour >= 19 & hour <= 23) | (hour >= 0 & hour <= 6))
    
    # Group by 'device_aid', 'combo_location' and calculate the total count
    rdd_4 <- rdd_3 %>% 
      group_by(device_aid, combo_location) %>% 
      summarise(total_count = n(), .groups = "drop") %>% 
      arrange(device_aid, combo_location) %>% 
      sdf_repartition(partitions = 1)
    
    # Set the save location for the output CSV file
    save_location <- file.path(wd, "home_location_new_1", day)
    
    # Write the final DataFrame to CSV
    spark_write_csv(rdd_4, save_location)
    
    # Disconnect from Spark
    spark_disconnect(sc)
}
```

```{}
```
