---
title: "Analysing home location"
format: html
editor: visual
---

# Clean environment

```{r}
rm(list=ls())
```

# Libraries

```{r}
library(sparklyr)    
library(dplyr)       
library(glue)
library(future)
library(future.apply)
library(sf)   
library(ggplot2)
library(h3)
library(tidyverse)
library(fs)
```

# Directory

```{r}
wd <- "/Volumes/DEBIAS/data/inputs/"
```

# Parquetised

```{r}
# Month or week? If month == TRUE, work with data for March 2021. Else, work with data for first week of April 2021.

month <- FALSE
```

## Read input data

```{r}
# Helper: choose optimal partitions
choose_partitions <- function(path, cores) {
  # Force file size to numeric (not fs_bytes)
  total_size <- sum(as.numeric(file_info(dir_ls(path, recurse = TRUE, glob = "*.csv.gz"))$size))
  size_gb <- total_size / (1024^3)
  target_parts <- ceiling((size_gb * 1024) / 128)
  partitions <- min(max(target_parts, cores), cores * 5)
  message(glue("Data size: {round(size_gb, 2)} GB"))
  message(glue("Using {cores} cores, chosen {partitions} partitions"))
  return(as.integer(partitions))   
}

start_time <- Sys.time()

config <- spark_config()
config$`sparklyr.shell.driver-memory` <- '40G'
sc <- spark_connect(master = "local", config = config)

num_cores <- parallel::detectCores()

if (month == TRUE) {
  input_path_home <- file.path(wd, "mapp1", "home", "location-home_month")
} else {
  input_path_home <- file.path(wd, "mapp1", "home", "location-home")
}
optimal_parts_home <- choose_partitions(input_path_home, num_cores)
optimal_parts_home <- as.integer(optimal_parts_home)   
df_home <- spark_read_parquet(sc, path = input_path_home)
df_home <- sdf_repartition(df_home, partitions = optimal_parts_home)
df_home_location <- df_home %>% collect()


if (month == TRUE) {
  input_path_work <- file.path(wd, "mapp1", "work", "location-work_month")
} else {
  input_path_work <- file.path(wd, "mapp1", "work", "location-work")
}
optimal_parts_work <- choose_partitions(input_path_work, num_cores)
optimal_parts_work <- as.integer(optimal_parts_work)   
df_work <- spark_read_parquet(sc, path = input_path_work)
df_work <- sdf_repartition(df_work, partitions = optimal_parts_work)
df_work_location <- df_work %>% collect()

# Disconnect from Spark
spark_disconnect(sc)

# End timer
end_time <- Sys.time()

print(end_time - start_time)
```

```{r}
lookup <- read.csv("/Volumes/DEBIAS/data/inputs/geographies/lookup21ew/hex10-lsoa-msoa-lad.csv") %>%
  dplyr::select(-c(X))
```

## Find home and work location for device

We keep only devices where we are able to detect both home and work locations.

```{r}
df_home_aux <- df_home_location %>% 
  rename(device_aid_home = device_aid, location_home = location) %>%
  select(device_aid_home, location_home)

df_work_aux <- df_work_location %>% 
  rename(device_aid_work = device_aid, location_work = location) %>%
  select(device_aid_work, location_work)

df_hw <- inner_join(df_home_aux, df_work_aux, by=c('device_aid_home' = 'device_aid_work'))
```

# Add geography codes

```{r}
df_hw_2 <- df_hw %>%
  left_join(lookup,
        by = c("location_home" = "hex_id")
        ) %>%
  rename("LAD22CD_home" = "LAD22CD",
         "MSOA21CD_home" = "MSOA21CD",
         "LSOA21CD_home" = "LSOA21CD") %>%
  left_join(lookup,
        by = c("location_work" = "hex_id")
        ) %>%
  rename("LAD22CD_work" = "LAD22CD",
         "MSOA21CD_work" = "MSOA21CD",
         "LSOA21CD_work" = "LSOA21CD") 
  
```


## Group by LAD home and work

```{r}
df_flows_lad <- df_hw_2 %>%
  select(c("LAD22CD_home", "LAD22CD_work"))
  group_by(LAD22CD_home, LAD22CD_work) %>%
  summarise(count = n(), .groups = "drop")

if (month == TRUE) {
  write.csv(df_flows, "/Volumes/DEBIAS/data/inputs/mapp1/flows/lad/flow-home-work_lad_htw_pw-month.csv")
} else {
  write.csv(df_flows, "/Volumes/DEBIAS/data/inputs/mapp1/flows/lad/flow-home-work_lad_htw_pw.csv")
}
```

## Group by MSOA home and work

```{r}
df_flows_msoa <- df_hw_2 %>%
  select(c("MSOA21CD_home", "MSOA21CD_work"))
  group_by(MSOA21CD_home, MSOA21CD_work) %>%
  summarise(count = n(), .groups = "drop")

if (month == TRUE) {
  write.csv(df_flows, "/Volumes/DEBIAS/data/inputs/mapp1/flows/msoa/flow-home-work_msoa_htw_pw-month.csv")
} else {
  write.csv(df_flows, "/Volumes/DEBIAS/data/inputs/mapp1/flows/msoa/flow-home-work_msoa_htw_pw.csv")
}
```


## Group by LSOA home and work

```{r}
df_flows <- df_hw_2 %>%
  select(c("LSOA21CD_home", "LSOA21CD_work"))
  group_by(LSOA21CD_home, LSOA21CD_work)) %>%
  summarise(count = n(), .groups = "drop")

if (month == TRUE) {
  write.csv(df_flows, "/Volumes/DEBIAS/data/inputs/mapp1/flows/lsoa/flow-home-work_lsoa_htw_pw-month.csv")
} else {
  write.csv(df_flows, "/Volumes/DEBIAS/data/inputs/mapp1/flows/lsoa/flow-home-work_lsoa_htw_pw.csv")
}
```
