---
title: "07_pw_location-device_hour_frequency"
format: html
editor: visual
---

# Clean environment

```{r}
rm(list=ls())
```

# Libraries

```{r}
# Load libraries
library(sparklyr)    # For connecting to Spark
library(dplyr)       # For data manipulation
library(glue)
library(future)
library(future.apply)
library(fs)   # for file size detection
```

# Directory

```{r}
wd <- "/Volumes/DEBIAS/data/inputs/pickwell/uk"
```

# Select dates

```{r}
days <-  c(
  "01042021",
  "02042021",
  "03042021",
  "04042021",
  "05042021",
  "06042021",
  "07042021"
)

```

Uncomment if testing

```{r}
# days <-  c(
#   "01042021")
```

# Identifying unique device-location pairs

## Parallelised

Processing took 18.85 seconds

```{r}
start_time <- Sys.time()

config <- spark_config()
config$`sparklyr.shell.driver-memory` <- '40G'
sc <- spark_connect(master = "local[*]", config = config)

num_cores <- parallel::detectCores()
optimal_parts <- num_cores * 3

file_pattern <- file.path(wd, "geocoded-hex10", "*", "*-geocoded.csv.gz")
df_all <- spark_read_csv(
  sc,
  path = file_pattern,
  memory = TRUE,
  repartition = optimal_parts
) %>%
  select(device_aid, code_h3, hour) %>%  # drop unused columns
  mutate(day = regexp_extract(input_file_name(), ".*/geocoded-hex10/([^/]+)/.*", 1))

df_all <- df_all %>%
  mutate(day = regexp_extract(input_file_name(), ".*/geocoded-hex10/([^/]+)/.*", 1))

rdd_all <- df_all %>%
  mutate(location = code_h3) %>%
  filter(hour >= 22 | hour <= 6) %>%
  group_by(day, device_aid, location) %>%
  summarise(total_count = n(), .groups = "drop") %>%
  sdf_repartition(partitions = optimal_parts)  # for writing efficiency

spark_write_csv(
  rdd_all,
  path = file.path(wd, "device-locations_22t07_parallelised"),
  partition_by = "day",
  mode = "overwrite",
  compression = "gzip"  # smaller + faster writing
)

spark_disconnect(sc)

print(glue("Processing took: {round(Sys.time() - start_time, 2)} seconds"))
```

## Non-parallelised

Processing took 16.64 seconds

```{r}
# Start timer
start_parallel <- Sys.time()

for (day in days) {

    # Configure Spark
    config <- spark_config()
    config$`sparklyr.shell.driver-memory` <- '40G'
    
    # Connect to Spark
    sc <- spark_connect(master = "local", config = config)
    
    # Define the data parent folder and file pattern
    data_parent_folder <- file.path(
      wd, "geocoded-hex10", day
      )
    file_pattern <- file.path(data_parent_folder, "*-geocoded.csv.gz")
    
    # Read CSV files into Spark DataFrame
    df <- spark_read_csv(
      sc,
      path = file_pattern,
      memory = TRUE
    )
    
    # Create a new location column labelled `location` based on the hex3_code
    rdd_1 <- df %>% 
      mutate(
        location = code_h3
      )
    
    # Filter for rows where the time is between '19:00:00' and '6:59:00'
    rdd_2 <- rdd_1 %>%
      filter((hour >= 22 & hour <= 23) | (hour >= 0 & hour <= 6))
    
    # Group by 'device_aid', 'location' and calculate the total count
    rdd_3 <- rdd_2 %>% 
      group_by(device_aid, location) %>% 
      summarise(total_count = n(), .groups = "drop") %>% 
      arrange(device_aid, location) %>% 
      sdf_repartition(partitions = 1)
    
    # Set the save location for the output CSV file
    save_location <- file.path(wd, "device-locations_22t07_non-parallelised", day)
    
    # Write the final DataFrame to CSV
    spark_write_csv(rdd_3, save_location)
    
    # Disconnect from Spark
    spark_disconnect(sc)
}

# End timer
end_parallel <- Sys.time()

# Report
parallel_duration <- end_parallel - start_parallel
print(glue::glue("Non-parallelised version took: {round(parallel_duration, 2)} seconds"))
```

```{r}
# df_location <- read.csv("/Volumes/DEBIAS/data/inputs/pickwell/uk/home_location_new_1/01042021/part-00000-4b5d8191-a38c-4989-952a-917163da15b4-c000.csv")
```

## Alternative

```{r}

parquet_path <- file.path(wd, "geocoded-hex10_parquet-test")
if (dir_exists(parquet_path)) dir_delete(parquet_path)  # Clean up before writing

# Helper: choose optimal partitions
choose_partitions <- function(path, cores) {
  # Force file size to numeric (not fs_bytes)
  total_size <- sum(as.numeric(file_info(dir_ls(path, recurse = TRUE, glob = "*.csv.gz"))$size))
  size_gb <- total_size / (1024^3)
  target_parts <- ceiling((size_gb * 1024) / 128)
  partitions <- min(max(target_parts, cores), cores * 5)
  message(glue("Data size: {round(size_gb, 2)} GB"))
  message(glue("Using {cores} cores, chosen {partitions} partitions"))
  return(as.integer(partitions))   # <<--- Ensure return is integer!
}

num_cores <- parallel::detectCores()
optimal_parts <- choose_partitions(file.path(wd, "geocoded-hex10"), num_cores)
optimal_parts <- as.integer(optimal_parts)   # <<--- Safety, force integer again


config <- spark_config()
config$`sparklyr.shell.driver-memory` <- '8G'
sc <- spark_connect(master = "local", config = config)

df_csv <- spark_read_csv(
  sc,
  path = file.path(wd, "geocoded-hex10", "*", "*-geocoded.csv.gz"),
  memory = FALSE
  ) %>%
  select(device_aid, code_h3, hour) %>%
  mutate(day = regexp_extract(input_file_name(), ".*/geocoded-hex10/([^/]+)/.*", 1)) 

df_csv <- sdf_repartition(df_csv, partitions = optimal_parts)


spark_write_parquet(df_csv, path = parquet_path, mode = "overwrite")

```

```{r}

# Helper: choose optimal partitions
choose_partitions <- function(path, cores) {
  # Force file size to numeric (not fs_bytes)
  total_size <- sum(as.numeric(file_info(dir_ls(path, recurse = TRUE, glob = "*.csv.gz"))$size))
  size_gb <- total_size / (1024^3)
  target_parts <- ceiling((size_gb * 1024) / 128)
  partitions <- min(max(target_parts, cores), cores * 5)
  message(glue("Data size: {round(size_gb, 2)} GB"))
  message(glue("Using {cores} cores, chosen {partitions} partitions"))
  return(as.integer(partitions))   # <<--- Ensure return is integer!
}

num_cores <- parallel::detectCores()
optimal_parts <- choose_partitions(file.path(wd, "geocoded-hex10_parquet-test"), num_cores)
optimal_parts <- as.integer(optimal_parts)   # <<--- Safety, force integer again

start_time <- Sys.time()

df_all <- spark_read_parquet(sc, path = parquet_path)
df_all <- sdf_repartition(df_all, partitions = optimal_parts)

# Main processing
rdd_all <- df_all %>%
  mutate(location = code_h3) %>%
  filter(hour >= 22 | hour <= 6) %>%
  group_by(day, device_aid, location) %>%
  summarise(total_count = n(), .groups = "drop")

# Write output, partitioned by day, compressed
output_path <- file.path(wd, "device-locations_22t07_parquet-test")
if (dir_exists(output_path)) fs::dir_delete(output_path)

spark_write_parquet(
  rdd_all,
  path = output_path,
  partition_by = "day",
  mode = "overwrite"
)


spark_disconnect(sc)

end_time <- Sys.time()
message(glue("TOTAL runtime: {round(end_time - start_time, 2)} seconds"))
```

```         
Data size: 0 GB
Using 10 cores, chosen 10 partitions
TOTAL runtime: 5.95 seconds
```
