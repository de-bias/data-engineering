---
title: "07_pw_location-device_hour_frequency"
format: html
editor: visual
---

# Clean environment

```{r}
rm(list=ls())
```

# Libraries

```{r}
# Load libraries
library(sparklyr)    # For connecting to Spark
library(dplyr)       # For data manipulation
```

# Directory

```{r}
wd <- "/Volumes/DEBIAS/data/inputs/pickwell/uk"
```

# Select dates

```{r}
days <-  c(
  "01042021",
  "02042021",
  "03042021",
  "04042021",
  "05042021",
  "06042021",
  "07042021"
)

```

Uncomment if testing

```{r}
# days <-  c(
#   "01042021")
```

# Identifying unique device-location pairs

## Parallelised

```{r}
# Set parallel backend for macOS
plan(multisession, workers = parallel::detectCores())
```

```{r}
# Function to process a single day
process_day <- function(day) {
  date_obj <- as.Date(day, format = "%d%m%Y")
  day_name <- weekdays(date_obj)
  
  message(sprintf("Processing day: %s (%s)", day, day_name))
  
  # Spark config
  config <- spark_config()
  config$`sparklyr.shell.driver-memory` <- '40G'
  
  # Connect to Spark
  sc <- spark_connect(master = "local", config = config)
  
  # File paths
  data_parent_folder <- file.path(wd, "geocoded-hex10", day)
  file_pattern <- file.path(data_parent_folder, "*-geocoded.csv.gz")
  
  # Read and transform data
  df <- spark_read_csv(sc, path = file_pattern, memory = TRUE)
  
  rdd_3 <- df %>%
    mutate(location = code_h3) %>%
    filter((hour >= 22 & hour <= 23) | (hour >= 0 & hour <= 6)) %>%
    group_by(device_aid, location) %>%
    summarise(total_count = n(), .groups = "drop") %>%
    arrange(device_aid, location) %>%
    sdf_repartition(partitions = 1)
  
  # Save output
  save_location <- file.path(wd, "device-locations_22t07", day)
  spark_write_csv(rdd_3, save_location)
  
  spark_disconnect(sc)
  
  return(day)
}

# Run all days in parallel
results <- future_lapply(days, process_day)
```

## Non-parallelised

```{r}
for (day in days) {

    # Configure Spark
    config <- spark_config()
    config$`sparklyr.shell.driver-memory` <- '40G'
    
    # Connect to Spark
    sc <- spark_connect(master = "local", config = config)
    
    # Define the data parent folder and file pattern
    data_parent_folder <- file.path(
      wd, "geocoded-hex10", day
      )
    file_pattern <- file.path(data_parent_folder, "*-geocoded.csv.gz")
    
    # Read CSV files into Spark DataFrame
    df <- spark_read_csv(
      sc,
      path = file_pattern,
      memory = TRUE
    )
    
    # Create a new location column labelled `location` based on the hex3_code
    rdd_1 <- df %>% 
      mutate(
        location = code_h3
      )
    
    # Filter for rows where the time is between '19:00:00' and '6:59:00'
    rdd_2 <- rdd_1 %>%
      filter((hour >= 22 & hour <= 23) | (hour >= 0 & hour <= 6))
    
    # Group by 'device_aid', 'location' and calculate the total count
    rdd_3 <- rdd_2 %>% 
      group_by(device_aid, location) %>% 
      summarise(total_count = n(), .groups = "drop") %>% 
      arrange(device_aid, location) %>% 
      sdf_repartition(partitions = 1)
    
    # Set the save location for the output CSV file
    save_location <- file.path(wd, "device-locations_22t07", day)
    
    # Write the final DataFrame to CSV
    spark_write_csv(rdd_3, save_location)
    
    # Disconnect from Spark
    spark_disconnect(sc)
}
```

```{r}
# df_location <- read.csv("/Volumes/DEBIAS/data/inputs/pickwell/uk/home_location_new_1/01042021/part-00000-4b5d8191-a38c-4989-952a-917163da15b4-c000.csv")
```
