---
title: "Identifying home location"
format: html
editor: visual
---

# Clean environment

```{r}
rm(list=ls())
```

# Libraries

```{r}
# Load libraries
library(sparklyr)    # For connecting to Spark
library(dplyr)       # For data manipulation
library(glue)
library(future)
library(future.apply)
library(fs)   # for file size detection
```

# Directory

```{r}
wd <- "/Volumes/DEBIAS/data/inputs/pickwell/uk"
```

# Home locations

## Non-parallelised

```{r}
# Configure Spark
config <- spark_config()
config$`sparklyr.shell.driver-memory` <- '40G'

# Connect to Spark
sc <- spark_connect(master = "local", config = config)
```

```{r}
# Define the data parent folder and file pattern 
data_parent_folder <- file.path(wd, "device-locations-home_time-aggregation") 

file_pattern <- file.path(data_parent_folder, "*.csv")

# Read CSV files into Spark DataFrame

df <- spark_read_csv( sc, path = file_pattern, memory = TRUE )

# Determine if a location is considered home based on the percentage

rdd_1 <- df %>%
  mutate(
    is_home = case_when( 
      percent > 50 & overall_count > 2 ~ TRUE,
      TRUE ~ FALSE )
    )

# Filter for rows where 'is_home' is TRUE and repartition the DataFrame

rdd_2 <- rdd_1 %>% 
  filter(is_home == TRUE) %>% 
  sdf_repartition(partitions = 1)

save_location <- file.path(wd, "location-home")

# Write the final DataFrame to CSV

spark_write_csv(rdd_2, save_location)

# Disconnect from Spark

spark_disconnect(sc)

```

## Alternative

```{r}
# Helper: choose optimal partitions
choose_partitions <- function(path, cores) {
  # Force file size to numeric (not fs_bytes)
  total_size <- sum(as.numeric(file_info(dir_ls(path, recurse = TRUE, glob = "*.csv.gz"))$size))
  size_gb <- total_size / (1024^3)
  target_parts <- ceiling((size_gb * 1024) / 128)
  partitions <- min(max(target_parts, cores), cores * 5)
  message(glue("Data size: {round(size_gb, 2)} GB"))
  message(glue("Using {cores} cores, chosen {partitions} partitions"))
  return(as.integer(partitions))   # <<--- Ensure return is integer!
}

input_path <- file.path(wd, "device-locations-home_time-aggregation_parquet-test")

num_cores <- parallel::detectCores()
optimal_parts <- choose_partitions(input_path, num_cores)
optimal_parts <- as.integer(optimal_parts)   # <<--- Safety, force integer again

start_time <- Sys.time()

config <- spark_config()
config$`sparklyr.shell.driver-memory` <- '8G'
sc <- spark_connect(master = "local", config = config)

df <- spark_read_parquet(sc, path = input_path)
df <- sdf_repartition(df, partitions = optimal_parts)

# Determine if a location is considered home based on the percentage
rdd_1 <- df %>%
  mutate(
    is_home = case_when( 
      percent > 50 & overall_count > 2 ~ TRUE,
      TRUE ~ FALSE )
    )

# Filter for rows where 'is_home' is TRUE and repartition the DataFrame
rdd_2 <- rdd_1 %>% 
  filter(is_home == TRUE) %>% 
  sdf_repartition(partitions = 1)

output_path <- file.path(wd, "location-home_parquet-test")
if (dir_exists(output_path)) dir_delete(output_path)  # Clean up before writing

# Write the final parquet
spark_write_parquet(rdd_2, output_path)

# Disconnect from Spark
spark_disconnect(sc)

# End timer
end_time <- Sys.time()

print(end_time - start_time)

```
