---
title: "Measuring overall time share of a device-location pair"
format: html
editor: visual
---

# Clean environment

```{r}
rm(list=ls())
```

# Libraries

```{r}
# Load libraries
library(sparklyr)    # For connecting to Spark
library(dplyr)       # For data manipulation
library(glue)
library(future)
library(future.apply)
library(fs)   # for file size detection
```

# Directory

```{r}
wd <- "/Volumes/DEBIAS/data/inputs/"
```

# Share of counts in each location for each device

## Parquetised

```{r}

# Helper: choose optimal partitions
choose_partitions <- function(path, cores) {
  # Force file size to numeric (not fs_bytes)
  total_size <- sum(as.numeric(file_info(dir_ls(path, recurse = TRUE, glob = "*.csv.gz"))$size))
  size_gb <- total_size / (1024^3)
  target_parts <- ceiling((size_gb * 1024) / 128)
  partitions <- min(max(target_parts, cores), cores * 5)
  message(glue("Data size: {round(size_gb, 2)} GB"))
  message(glue("Using {cores} cores, chosen {partitions} partitions"))
  return(as.integer(partitions))   # <<--- Ensure return is integer!
}

input_path <- file.path(wd, "mapp1", "home", "device-locations_22t07")

num_cores <- parallel::detectCores()
optimal_parts <- choose_partitions(input_path, num_cores)
optimal_parts <- as.integer(optimal_parts)   # <<--- Safety, force integer again

start_time <- Sys.time()

config <- spark_config()
config$`sparklyr.shell.driver-memory` <- '8G'
sc <- spark_connect(master = "local", config = config)

df <- spark_read_parquet(sc, path = input_path)
df <- sdf_repartition(df, partitions = optimal_parts)

# Group by 'device_aid' and 'location', and calculate the sum of 'total_count'
rdd_1 <- df %>% 
  group_by(device_aid, location) %>% 
  summarise(overall_count = sum(total_count), .groups = "drop") %>% 
  arrange(device_aid, location)

# Group by 'device_aid', and calculate the percentage based on 'overall_count'
rdd_2 <- rdd_1 %>% 
  group_by(device_aid) %>% 
  mutate(percent = 100 * (overall_count / sum(overall_count))) %>% 
  ungroup() %>% 
  arrange(device_aid, location)

output_path <- file.path(wd, "mapp1", "home", "device-locations-home_time-aggregation")
if (dir_exists(output_path)) dir_delete(output_path)  # Clean up before writing

# Write the final parquet
spark_write_parquet(rdd_2, output_path)

# Disconnect from Spark
spark_disconnect(sc)

# End timer
end_time <- Sys.time()

print(end_time - start_time)
```

## Non-parquetised experiments

### Parallelised

```{r}
# # Start timer
# start_parallel <- Sys.time()
# 
# # Set parallel plan
# plan(multisession, workers = parallel::detectCores())
# 
# parent_path <- file.path(wd, "device-locations_22t07_parallelised")
# 
# # List all subfolders (days)
# day_folders <- list.dirs(parent_path, full.names = TRUE, recursive = FALSE)
# 
# # Function to process a single folder
# process_folder <- function(folder_path) {
#   config <- spark_config()
#   config$`sparklyr.shell.driver-memory` <- '10G'  # reduce if many workers
#   
#   sc <- spark_connect(master = "local", config = config)
#   
#   # Read CSVs in this day's folder
#   df <- spark_read_csv(sc, path = file.path(folder_path, "*.csv"), memory = TRUE)
#   
#   # Summarise by device_aid and location
#   out <- df %>%
#     group_by(device_aid, location) %>%
#     summarise(total_count = sum(total_count), .groups = "drop") %>%
#     collect()
#   
#   spark_disconnect(sc)
#   return(out)
# }
# 
# # Run in parallel for each day's folder
# partial_results <- future_lapply(day_folders, process_folder)
# 
# # Combine all partial data frames into one
# combined <- bind_rows(partial_results)
# 
# # Final aggregation and percent computation
# final_result <- combined %>%
#   group_by(device_aid, location) %>%
#   summarise(overall_count = sum(total_count), .groups = "drop") %>%
#   group_by(device_aid) %>%
#   mutate(percent = 100 * (overall_count / sum(overall_count))) %>%
#   ungroup() %>%
#   arrange(device_aid, location)
# 
# # Save final result
# write.csv(
#   final_result,
#   file.path(wd, "device-locations-home_time-aggregation_parallelised/device-locations-home_time-aggregation.csv"),
#   row.names = FALSE
# )
# 
# # End timer
# end_parallel <- Sys.time()
# 
# # Report
# parallel_duration <- end_parallel - start_parallel
# print(glue::glue("Parallelised version took: {round(parallel_duration, 2)} seconds"))
```

### Non-parallelised

```{r}
# # Start timer
# start_parallel <- Sys.time()
# 
# # Configure Spark
# config <- spark_config()
# config$`sparklyr.shell.driver-memory` <- '40G'
# 
# # Connect to Spark
# sc <- spark_connect(master = "local", config = config)
# 
# # Define the data parent folder and file pattern
# data_parent_folder <- file.path(wd, "device-locations_22t07_non-parallelised")
# file_pattern <- file.path(data_parent_folder, "*", "*.csv")
# 
# # Read CSV files into Spark DataFrame
# df <- spark_read_csv(
#   sc,
#   path = file_pattern,
#   memory = TRUE
# )
# 
# # Group by 'device_aid' and 'location', and calculate the sum of 'total_count'
# rdd_1 <- df %>% 
#   group_by(device_aid, location) %>% 
#   summarise(overall_count = sum(total_count), .groups = "drop") %>% 
#   arrange(device_aid, location)
# 
# # Group by 'device_aid', and calculate the percentage based on 'overall_count'
# rdd_2 <- rdd_1 %>% 
#   group_by(device_aid) %>% 
#   mutate(percent = 100 * (overall_count / sum(overall_count))) %>% 
#   ungroup() %>% 
#   arrange(device_aid, location)
# 
# save_location <- file.path(wd, "device-locations-home_time-aggregation_non-parallelised")
# 
# # Write the final DataFrame to CSV
# spark_write_csv(rdd_2, save_location)
# 
# # Disconnect from Spark
# spark_disconnect(sc)
# 
# # End timer
# end_parallel <- Sys.time()
# 
# # Report
# parallel_duration <- end_parallel - start_parallel
# print(glue::glue("Non-parallelised version took: {round(parallel_duration, 2)} seconds"))
```

```{r}
# df_location <- read.csv("/Volumes/DEBIAS/data/inputs/pickwell/uk/device-locations_time-aggregation/part-00000-5bc1254d-be52-46de-a616-4ecc418aaee2-c000.csv")
```
