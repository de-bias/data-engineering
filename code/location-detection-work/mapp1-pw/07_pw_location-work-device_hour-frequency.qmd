---
title: "07_pw_location_work-device_hour_frequency"
format: html
editor: visual
---

# Clean environment

```{r}
rm(list=ls())
```

# Libraries

```{r}
# Load libraries
library(sparklyr)    # For connecting to Spark
library(dplyr)       # For data manipulation
library(future)
library(future.apply)
```

# Directory

```{r}
wd <- "/Volumes/DEBIAS/data/inputs/pickwell/uk"
```

# Select dates

```{r}
days <-  c(
  "01042021",
  "02042021",
  "03042021",
  "04042021",
  "05042021",
  "06042021",
  "07042021"
)

```

Uncomment if testing

```{r}
# days <-  c(
#   "01042021")
```

# Identifying unique device-location pairs

## Parallelised

```{r}
# Set parallel backend for macOS
plan(multisession, workers = parallel::detectCores())
```

```{r}
# Function to process a single day
process_day <- function(day) {
  date_obj <- as.Date(day, format = "%d%m%Y")
  day_name <- weekdays(date_obj)
  
  if (day_name %in% c("Saturday", "Sunday")) {
    message(sprintf("Skipping weekend: %s (%s)", day, day_name))
    return(NULL)
  }
  
  message(sprintf("Processing day: %s (%s)", day, day_name))
  
  # Spark config
  config <- spark_config()
  config$`sparklyr.shell.driver-memory` <- '40G'
  
  # Connect to Spark
  sc <- spark_connect(master = "local", config = config)
  
  # File paths
  data_parent_folder <- file.path(wd, "geocoded-hex10", day)
  file_pattern <- file.path(data_parent_folder, "*-geocoded.csv.gz")
  
  # Read and transform data
  df <- spark_read_csv(sc, path = file_pattern, memory = TRUE)
  
  rdd_3 <- df %>%
    mutate(location = code_h3) %>%
    filter(hour >= 9 & hour <= 16) %>%
    group_by(device_aid, location) %>%
    summarise(total_count = n(), .groups = "drop") %>%
    arrange(device_aid, location) %>%
    sdf_repartition(partitions = 1)
  
  # Save output
  save_location <- file.path(wd, "device-locations_09t16", day)
  spark_write_csv(rdd_3, save_location)
  
  spark_disconnect(sc)
  
  return(day)
}

# Run all days in parallel
results <- future_lapply(days, process_day)
```


## Non-parallelised

```{r}
for (day in days) {

    # Convert to Date object (format = day-month-year)
    date_obj <- as.Date(day, format = "%d%m%Y")

    # Get the weekday name
    day_name <- weekdays(date_obj)

    # Check if it's weekend
    is_weekend <- day_name %in% c("Saturday", "Sunday")


    if (!is_weekend) {

        # Configure Spark
        config <- spark_config()
        config$`sparklyr.shell.driver-memory` <- '40G'
        
        # Connect to Spark
        sc <- spark_connect(master = "local", config = config)
        
        # Define the data parent folder and file pattern
        data_parent_folder <- file.path(
          wd, "geocoded-hex10", day
          )
        file_pattern <- file.path(data_parent_folder, "*-geocoded.csv.gz")
        
        # Read CSV files into Spark DataFrame
        df <- spark_read_csv(
          sc,
          path = file_pattern,
          memory = TRUE
        )
        
        # Create a new location column labelled `location` based on the hex3_code
        rdd_1 <- df %>% 
          mutate(
            location = code_h3
          )
        
        # Filter for rows where the time is between '09:00:00' and '16:00:00'
        rdd_2 <- rdd_1 %>%
          filter((hour >= 9 & hour <= 16))
        
        # Group by 'device_aid', 'location' and calculate the total count
        rdd_3 <- rdd_2 %>% 
          group_by(device_aid, location) %>% 
          summarise(total_count = n(), .groups = "drop") %>% 
          arrange(device_aid, location) %>% 
          sdf_repartition(partitions = 1)
        
        # Set the save location for the output CSV file
        save_location <- file.path(wd, "device-locations_09t16", day)
        
        # Write the final DataFrame to CSV
        spark_write_csv(rdd_3, save_location)
        
        # Disconnect from Spark
        spark_disconnect(sc)
    
    }
}
```

```{r}
# df_location <- read.csv("/Volumes/DEBIAS/data/inputs/pickwell/uk/home_location_new_1/01042021/part-00000-4b5d8191-a38c-4989-952a-917163da15b4-c000.csv")
```
