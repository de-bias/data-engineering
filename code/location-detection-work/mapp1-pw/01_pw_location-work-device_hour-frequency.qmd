---
title: "07_pw_location-device_hour_frequency"
format: html
editor: visual
---

# Clean environment

```{r}
rm(list=ls())
```

# Libraries

```{r}
# Load libraries
library(sparklyr)    # For connecting to Spark
library(dplyr)       # For data manipulation
library(glue)
library(future)
library(future.apply)
library(fs)   # for file size detection
```

# Directory

```{r}
wd <- "/Volumes/DEBIAS/data/inputs/"
```

# Identifying unique device-location pairs

```{r}
# Month or week? If month == TRUE, work with data for March 2021. Else, work with data for first week of April 2021.

month <- TRUE
```

## Parquetised

Careful with code chunk below, only uncomment if wanting to transform csv hex locations to parquet. Takes a while and only needs to be done once (or alternatively, modify 06 to write files as parquet directly).

```{r}

# if (month == TRUE) {
#   parquet_path <- file.path(wd, "mapp1", "geocoded-hex10-month")
# if (dir_exists(parquet_path)) dir_delete(parquet_path) 
# } else {
#   parquet_path <- file.path(wd, "mapp1", "geocoded-hex10")
# if (dir_exists(parquet_path)) dir_delete(parquet_path)
# }
# 
# # Helper: choose optimal partitions
# choose_partitions <- function(path, cores) {
#   # Force file size to numeric (not fs_bytes)
#   total_size <- sum(as.numeric(file_info(dir_ls(path, recurse = TRUE, glob = "*.csv.gz"))$size))
#   size_gb <- total_size / (1024^3)
#   target_parts <- ceiling((size_gb * 1024) / 128)
#   partitions <- min(max(target_parts, cores), cores * 5)
#   message(glue("Data size: {round(size_gb, 2)} GB"))
#   message(glue("Using {cores} cores, chosen {partitions} partitions"))
#   return(as.integer(partitions))   
# }
# 
# num_cores <- parallel::detectCores()
# optimal_parts <- choose_partitions(file.path(wd, "/aux/geocoded-hex10_aux"), num_cores)
# optimal_parts <- as.integer(optimal_parts)   
# 
# 
# config <- spark_config()
# config$`sparklyr.shell.driver-memory` <- '8G'
# sc <- spark_connect(master = "local", config = config)
# 
# if (month == TRUE) {
#     df_csv <- spark_read_csv(
#     sc,
#     path = file.path(wd, "mapp1/aux/geocoded-hex10-month_aux", "*", "*-geocoded.csv.gz"),
#     memory = FALSE
#     ) %>%
#     select(device_aid, code_h3, hour)  %>%
#       mutate(day = regexp_extract(input_file_name(), ".*/geocoded-hex10-month_aux/([^/]+)/.*", 1))
# } else {
#   df_csv <- spark_read_csv(
#     sc,
#     path = file.path(wd, "mapp1/aux/geocoded-hex10_aux", "*", "*-geocoded.csv.gz"),
#     memory = FALSE
#     ) %>%
#     select(device_aid, code_h3, hour) %>%
#     mutate(day = regexp_extract(input_file_name(), ".*/geocoded-hex10_aux/([^/]+)/.*", 1))
# }
# 
# df_csv <- sdf_repartition(df_csv, partitions = optimal_parts)
# 
# 
# spark_write_parquet(df_csv, path = parquet_path, mode = "overwrite")

```

Code below identifies unique device-location pairs, and retains day hours.

```{r}
if (month == TRUE) {
  parquet_path <- file.path(wd, "mapp1", "geocoded-hex10-month")
} else {
  parquet_path <- file.path(wd, "mapp1", "geocoded-hex10")
}

# Helper: choose optimal partitions
choose_partitions <- function(path, cores) {
  # Force file size to numeric (not fs_bytes)
  total_size <- sum(as.numeric(file_info(dir_ls(path, recurse = TRUE, glob = "*.csv.gz"))$size))
  size_gb <- total_size / (1024^3)
  target_parts <- ceiling((size_gb * 1024) / 128)
  partitions <- min(max(target_parts, cores), cores * 5)
  message(glue("Data size: {round(size_gb, 2)} GB"))
  message(glue("Using {cores} cores, chosen {partitions} partitions"))
  return(as.integer(partitions)) 
}

num_cores <- parallel::detectCores()
if (month == TRUE) {
  optimal_parts <- choose_partitions(file.path(wd, "mapp1", "geocoded-hex10-month"), num_cores)
} else {
  optimal_parts <- choose_partitions(file.path(wd, "mapp1", "geocoded-hex10"), num_cores)
}
optimal_parts <- as.integer(optimal_parts)   

start_time <- Sys.time()

config <- spark_config()
config$`sparklyr.shell.driver-memory` <- '40G'
sc <- spark_connect(master = "local", config = config)

df_all <- spark_read_parquet(sc, path = parquet_path)
df_all <- sdf_repartition(df_all, partitions = optimal_parts)

# Create date and filter for weekdays only
df_weekdays <- df_all %>%
  mutate(
    day_formatted = paste0(substr(day, 5, 8), "-", substr(day, 3, 4), "-", substr(day, 1, 2)),
    day_date = to_date(day_formatted, 'yyyy-MM-dd'),
    day_of_week = date_format(day_date, "E")  
  ) %>% 
  filter(day_of_week %in% c("Mon", "Tue", "Wed", "Thu", "Fri")) %>%
  select(-day_formatted, -day_date, -day_of_week) 
  
# Main processing
rdd_all <- df_weekdays %>%
  mutate(location = code_h3) %>%
  filter(hour >= 9 & hour < 16) %>%
  group_by(day, device_aid, location) %>%
  summarise(total_count = n(), .groups = "drop")

# Write output, partitioned by day, compressed
if (month == TRUE) {
  output_path <- file.path(wd, "mapp1", "work" , "device-locations_09t16-month")
  if (dir_exists(output_path)) fs::dir_delete(output_path)
} else {
  output_path <- file.path(wd, "mapp1", "work" , "device-locations_09t16")
  if (dir_exists(output_path)) fs::dir_delete(output_path)
}

spark_write_parquet(
  rdd_all,
  path = output_path,
  partition_by = "day",
  mode = "overwrite"
)

spark_disconnect(sc)

end_time <- Sys.time()
message(glue("TOTAL runtime: {round(end_time - start_time, 2)} seconds"))
```

```         
Data size: 0 GB
Using 10 cores, chosen 10 partitions
* Using Spark: 3.5.0
TOTAL runtime: 31.54 seconds
```

## Non-parquetised experiments

### Select dates

```{r}
days <-  c(
  "01042021",
  "02042021",
  "03042021",
  "04042021",
  "05042021",
  "06042021",
  "07042021"
)

```

Uncomment if testing

```{r}
# days <-  c(
#   "01042021")
```

### Parallelised

Processing took 18.85 seconds

```{r}
# start_time <- Sys.time()
# 
# config <- spark_config()
# config$`sparklyr.shell.driver-memory` <- '8G'
# sc <- spark_connect(master = "local[*]", config = config)
# 
# num_cores <- parallel::detectCores()
# optimal_parts <- num_cores * 3
# 
# file_pattern <- file.path(wd, "geocoded-hex10", "*", "*-geocoded.csv.gz")
# df_all <- spark_read_csv(
#   sc,
#   path = file_pattern,
#   memory = TRUE,
#   repartition = optimal_parts
# ) %>%
#   select(device_aid, code_h3, hour) %>%  # drop unused columns
#   mutate(day = regexp_extract(input_file_name(), ".*/geocoded-hex10/([^/]+)/.*", 1))
# 
# df_all <- df_all %>%
#   mutate(day = regexp_extract(input_file_name(), ".*/geocoded-hex10/([^/]+)/.*", 1))
# 
# rdd_all <- df_all %>%
#   mutate(location = code_h3) %>%
#   filter(hour >= 22 | hour <= 6) %>%
#   group_by(day, device_aid, location) %>%
#   summarise(total_count = n(), .groups = "drop") %>%
#   sdf_repartition(partitions = optimal_parts)  # for writing efficiency
# 
# spark_write_csv(
#   rdd_all,
#   path = file.path(wd, "device-locations_22t07_parallelised"),
#   partition_by = "day",
#   mode = "overwrite",
#   compression = "gzip"  # smaller + faster writing
# )
# 
# spark_disconnect(sc)
# 
# print(glue("Processing took: {round(Sys.time() - start_time, 2)} seconds"))
```

### Non-parallelised

Processing took 16.64 seconds

```{r}
# # Start timer
# start_parallel <- Sys.time()
# 
# for (day in days) {
# 
#     # Configure Spark
#     config <- spark_config()
#     config$`sparklyr.shell.driver-memory` <- '40G'
#     
#     # Connect to Spark
#     sc <- spark_connect(master = "local", config = config)
#     
#     # Define the data parent folder and file pattern
#     data_parent_folder <- file.path(
#       wd, "geocoded-hex10", day
#       )
#     file_pattern <- file.path(data_parent_folder, "*-geocoded.csv.gz")
#     
#     # Read CSV files into Spark DataFrame
#     df <- spark_read_csv(
#       sc,
#       path = file_pattern,
#       memory = TRUE
#     )
#     
#     # Create a new location column labelled `location` based on the hex3_code
#     rdd_1 <- df %>% 
#       mutate(
#         location = code_h3
#       )
#     
#     # Filter for rows where the time is between '19:00:00' and '6:59:00'
#     rdd_2 <- rdd_1 %>%
#       filter((hour >= 22 & hour <= 23) | (hour >= 0 & hour <= 6))
#     
#     # Group by 'device_aid', 'location' and calculate the total count
#     rdd_3 <- rdd_2 %>% 
#       group_by(device_aid, location) %>% 
#       summarise(total_count = n(), .groups = "drop") %>% 
#       arrange(device_aid, location) %>% 
#       sdf_repartition(partitions = 1)
#     
#     # Set the save location for the output CSV file
#     save_location <- file.path(wd, "device-locations_22t07_non-parallelised", day)
#     
#     # Write the final DataFrame to CSV
#     spark_write_csv(rdd_3, save_location)
#     
#     # Disconnect from Spark
#     spark_disconnect(sc)
# }
# 
# # End timer
# end_parallel <- Sys.time()
# 
# # Report
# parallel_duration <- end_parallel - start_parallel
# print(glue::glue("Non-parallelised version took: {round(parallel_duration, 2)} seconds"))
```

```{r}
# df_location <- read.csv("/Volumes/DEBIAS/data/inputs/pickwell/uk/home_location_new_1/01042021/part-00000-4b5d8191-a38c-4989-952a-917163da15b4-c000.csv")
```
