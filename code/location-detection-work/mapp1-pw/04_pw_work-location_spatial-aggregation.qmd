---
title: "Analysing work location"
format: html
editor: visual
---

# Clean environment

```{r}
rm(list=ls())
```

# Libraries

```{r}
library(sparklyr)    # For connecting to Spark
library(dplyr)       # For data manipulation
library(glue)
library(future)
library(future.apply)
library(sf)   # for file size detection
library(ggplot2)
library(h3)
library(tidyverse)
```

# Directory

```{r}
wd <- "/Volumes/DEBIAS/data/inputs"
```

# Read geographies

```{r}
sdf_lad <- st_read("/Volumes/DEBIAS/data/inputs/geographies/boundaries/LAD_Dec_2021_GB_BFC_2022_simplified.gpkg") %>% 
  st_transform(crs = 4326)

sdf_msoa <- st_read("/Volumes/DEBIAS/data/inputs/geographies/boundaries/MSOA_Dec_2021_EW_BFC_V7.gpkg") %>% 
  st_transform(crs = 4326) %>% 
  select(c("MSOA21CD", "SHAPE")) %>%
  rename("code" = "MSOA21CD")
```

# Parquetised

```{r}
# Month or week? If month == TRUE, work with data for March 2021. Else, work with data for first week of April 2021.

month <- TRUE
```

## Read input data

```{r}
# Helper: choose optimal partitions
choose_partitions <- function(path, cores) {
  # Force file size to numeric (not fs_bytes)
  total_size <- sum(as.numeric(file_info(dir_ls(path, recurse = TRUE, glob = "*.csv.gz"))$size))
  size_gb <- total_size / (1024^3)
  target_parts <- ceiling((size_gb * 1024) / 128)
  partitions <- min(max(target_parts, cores), cores * 5)
  message(glue("Data size: {round(size_gb, 2)} GB"))
  message(glue("Using {cores} cores, chosen {partitions} partitions"))
  return(as.integer(partitions))   # <<--- Ensure return is integer!
}

if (month == TRUE) {
  input_path <- file.path(wd, "mapp1", "work", "location-work_month")
} else {
  input_path <- file.path(wd, "mapp1", "work", "location-work")
}

num_cores <- parallel::detectCores()
optimal_parts <- choose_partitions(input_path, num_cores)
optimal_parts <- as.integer(optimal_parts)   

start_time <- Sys.time()

config <- spark_config()
config$`sparklyr.shell.driver-memory` <- '40G'
sc <- spark_connect(master = "local", config = config)

df <- spark_read_parquet(sc, path = input_path)
df <- sdf_repartition(df, partitions = optimal_parts)

df_work_location <- df %>% collect()

# Disconnect from Spark
spark_disconnect(sc)

# End timer
end_time <- Sys.time()

print(end_time - start_time)
```

## Aggregate into geographies and save as gpkg

```{r}
# Create the histogram
ggplot(df_work_location, aes(x = percent)) +
  geom_histogram(fill = "skyblue", color = "black", bins = 50) +
  labs(title = "Histogram of Percent", x = "Percent", y = "Frequency") +
  theme_minimal()
```

```{r}
unique_vec <- unique(df_work_location$device_aid)
```

```{r}
coordinates <- h3_to_geo(df_work_location$location) %>% as_data_frame()
sdf_work_location <- cbind(df_work_location, coordinates) %>% 
  st_as_sf( coords = c("lng", "lat") , crs = 4326)

# Aggregate into LAD 

sdf_hex3 <- st_join(sdf_lad, 
                    sdf_work_location,
                    join = st_contains) 

sdf_hex3_lad <- sdf_hex3 %>% 
  group_by(code) %>% 
  summarise( work_location_count = n( ))

if (month == TRUE) {
  st_write(sdf_hex3_lad, "/Volumes/DEBIAS/data/inputs/mapp1/locations-work/lad/work-location-count_lda-month.gpkg")
} else {
  st_write(sdf_hex3_lad, "/Volumes/DEBIAS/data/inputs/mapp1/locations-work/lad/work-location-count_lda.gpkg")
}

# Aggregate into MSOA

sdf_hex3 <- st_join(sdf_msoa, 
                    sdf_work_location,
                    join = st_contains) 

sdf_hex3_msoa <- sdf_hex3 %>% 
  group_by(code) %>% 
  summarise( work_location_count = n( ))

if (month == TRUE) {
  st_write(sdf_hex3_msoa, "/Volumes/DEBIAS/data/inputs/mapp1/locations-work/msoa/work-location-count_msoa-month.gpkg")
} else {
  st_write(sdf_hex3_msoa, "/Volumes/DEBIAS/data/inputs/mapp1/locations-work/msoa/work-location-count_msoa.gpkg")
}
```

# Non-parquetised experiments

```{r}
# df_home_location_csv <- read_csv("/Volumes/DEBIAS/data/inputs/pickwell/uk/location-home/part-00000-22616738-1d83-45b0-9be8-2b01fc18da31-c000.csv")
# 
# coordinates <- h3_to_geo(df_home_location_csv$location) %>% as_data_frame()
# sdf_home_location <- cbind(df_home_location_csv, coordinates) %>%
#   st_as_sf( coords = c("lng", "lat") , crs = 4326)
# 
# sdf_hex3 <- st_join(sdf,
#                     sdf_home_location,
#                     join = st_contains)
# 
# sdf_lda <- sdf_hex3 %>%
#   group_by(code) %>%
#   summarise( home_location_count = n( ))
# 
# st_write(sdf_lda, "/Volumes/DEBIAS/data/outputs/mapp1/locations-home/lad/home-location-count_lda_non-parquetised.gpkg")
```
