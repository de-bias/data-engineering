---
title: "Analysing work location"
format: html
editor: visual
---

# Clean environment

```{r}
rm(list=ls())
```

# Libraries

```{r}
library(sparklyr)    # For connecting to Spark
library(dplyr)       # For data manipulation
library(glue)
library(future)
library(future.apply)
library(sf)   # for file size detection
library(ggplot2)
library(h3)
library(tidyverse)
```

# Directory

```{r}
wd <- "/Volumes/DEBIAS/data/inputs/pickwell/uk"
```

# Read geographies

```{r}
sdf <- st_read("/Volumes/DEBIAS/data/inputs/geographies/boundaries/LAD_Dec_2021_GB_BFC_2022_simplified.gpkg") %>% 
  st_transform(crs = 4326)
```

# Parquetised

## Read input data

```{r}
# Helper: choose optimal partitions
choose_partitions <- function(path, cores) {
  # Force file size to numeric (not fs_bytes)
  total_size <- sum(as.numeric(file_info(dir_ls(path, recurse = TRUE, glob = "*.csv.gz"))$size))
  size_gb <- total_size / (1024^3)
  target_parts <- ceiling((size_gb * 1024) / 128)
  partitions <- min(max(target_parts, cores), cores * 5)
  message(glue("Data size: {round(size_gb, 2)} GB"))
  message(glue("Using {cores} cores, chosen {partitions} partitions"))
  return(as.integer(partitions))   # <<--- Ensure return is integer!
}

input_path <- file.path(wd, "work", "location-work")

num_cores <- parallel::detectCores()
optimal_parts <- choose_partitions(input_path, num_cores)
optimal_parts <- as.integer(optimal_parts)   # <<--- Safety, force integer again

start_time <- Sys.time()

config <- spark_config()
config$`sparklyr.shell.driver-memory` <- '8G'
sc <- spark_connect(master = "local", config = config)

df <- spark_read_parquet(sc, path = input_path)
df <- sdf_repartition(df, partitions = optimal_parts)

df_work_location <- df %>% collect()

# Disconnect from Spark
spark_disconnect(sc)

# End timer
end_time <- Sys.time()

print(end_time - start_time)
```

## Aggregate into geographies and save as gpkg

```{r}
# Create the histogram
ggplot(df_work_location, aes(x = percent)) +
  geom_histogram(fill = "skyblue", color = "black", bins = 50) +
  labs(title = "Histogram of Percent", x = "Percent", y = "Frequency") +
  theme_minimal()
```

```{r}
unique_vec <- unique(df_work_location$device_aid)
```

```{r}
coordinates <- h3_to_geo(df_work_location$location) %>% as_data_frame()
sdf_work_location <- cbind(df_work_location, coordinates) %>% 
  st_as_sf( coords = c("lng", "lat") , crs = 4326)

sdf_hex3 <- st_join(sdf, 
                    sdf_work_location,
                    join = st_contains) 

sdf_lda <- sdf_hex3 %>% 
  group_by(code) %>% 
  summarise( work_location_count = n( ))

st_write(sdf_lda, "/Volumes/DEBIAS/data/outputs/mapp1/locations-home/lad/home-location-count_lda.gpkg")
```

## Non-parquetised experiments

```{r}
# df_home_location_csv <- read_csv("/Volumes/DEBIAS/data/inputs/pickwell/uk/location-home/part-00000-22616738-1d83-45b0-9be8-2b01fc18da31-c000.csv")
# 
# coordinates <- h3_to_geo(df_home_location_csv$location) %>% as_data_frame()
# sdf_home_location <- cbind(df_home_location_csv, coordinates) %>%
#   st_as_sf( coords = c("lng", "lat") , crs = 4326)
# 
# sdf_hex3 <- st_join(sdf,
#                     sdf_home_location,
#                     join = st_contains)
# 
# sdf_lda <- sdf_hex3 %>%
#   group_by(code) %>%
#   summarise( home_location_count = n( ))
# 
# st_write(sdf_lda, "/Volumes/DEBIAS/data/outputs/mapp1/locations-home/lad/home-location-count_lda_non-parquetised.gpkg")
```
